{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbef05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the data\n",
    "\n",
    "ratings_filename = \"dbfs:/mnt/Files/Validated/ratings.csv\"\n",
    "movies_filename = \"dbfs:/mnt/Files/Validated/movies.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks connecting (interact with the filesystems supported by the underlying Databricks cluster)\n",
    "%fs\n",
    "ls /mnt/Files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f4af4",
   "metadata": {},
   "source": [
    "## 2 Dataframes will be created for analysis\n",
    "1) movie_based_on_time - final schema (movie_id, name, Year)\n",
    "2) movie_based_on_genres - final schema (movie_id, name_with_year, one_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on movies.csv\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "movies_with_genres_df_schema = StructType(\n",
    "    [StructField('ID', IntegerType()),\n",
    "     StructField('title', StringType()),\n",
    "     StructField('genres', StringType())]\n",
    ")\n",
    "\n",
    "# dropping the genres, Year will be added later on transformation of df.\n",
    "movies_df_schema = StructType(\n",
    "    [StructField('ID', IntegerType()),\n",
    "     StructField('title', StringType())]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffa18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframes\n",
    "\n",
    "movies_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(movies_df_schema).load(movies_filename)\n",
    "movies_with_genres_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(movies_with_genres_df_schema).load(movies_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc4601",
   "metadata": {},
   "source": [
    "## Inspecting Dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7581e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.show(4, truncate= False) # for Collabrative filtering\n",
    "movies_with_genres_df.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the Dataframes\n",
    "from pyspark.sql.functions import split, regexp_extract\n",
    "\n",
    "movies_with_year_df = movies_df.select('ID', 'title',regexp_extract('title', '\\((\\d+)\\)',1).alias('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459876a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframes again\n",
    "movies_with_year_df.show(4, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here we can look at the count and find that the maximum number of movies are produced in 2009 \n",
    "display(movies_with_year_df.groupBy('year').count().orderBy('count',ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f549e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again for avoiding the action we are explicitly defining the schema, Time Stamp column dropped\n",
    "ratings_df_schema = StructType(\n",
    "    [StructField('userId', IntegerType()), \n",
    "     StructField('movieId', IntegerType()),\n",
    "     StructField('rating', DoubleType())]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the df\n",
    "ratings_df = sqlContext.read.format('com.databricks.spark.csv').options(header=True, inferSchema=False).schema(ratings_df_schema).load(ratings_filename)\n",
    "ratings_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fa70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache both the dataframes, because to avoid recomputing them each time\n",
    "ratings_df.cache()\n",
    "movies_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e3d7d",
   "metadata": {},
   "source": [
    "## Global Popularity (note: will discard the movies where the count of ratings is less than 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac27941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# From ratings DF, create a movie_ids_with_avg ratings_df that combines the two DataFrames \n",
    "movie_ids_with_avg ratings_df= ratings_df.groupBy('movieId').agg(F.count(ratings_df.rating).alias(\"count\"),\n",
    "F.avg(ratings_df.rating).alias(\"average\"))\n",
    "print('movie_ids_with_avg ratings_df:')\n",
    "movie_ids_with_avg ratings_df.show(4, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this df will have names with movie_id Make it more understandable\n",
    "movie_nanes_with_avg ratings_df= movie_ids_with_avg_ratings_df.join(movies_df,F.col('movieID') == F.col('ID')).drop('ID') \n",
    "movie_names_with_avg ratings_df.show(4, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ca4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global popularity\n",
    "movies_with_500_ratings_or_more = movie_names_with_avg ratings_df.filter(movie_names_with_avg ratings_df['count'] >= 500).orderBy('average', ascending = False)\n",
    "movies_with_500_ratings_or_more.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e96009",
   "metadata": {},
   "source": [
    "## Splitting in Train, Test and Validation dataset (tune parameters and test accuracy, test(check the final accuracy), validation(optimizing hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll hold out 60% for training, 20% of our data for validation, and leave 20% for testing \n",
    "\n",
    "seed = 4\n",
    "(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6,0.2,0.2], seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "training_df split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print('Training: {0}, validation: {1}, test: {2}\\n'.format( \n",
    "    training_df.count(), \n",
    "    validation_df.count(), \n",
    "    test_df.count())\n",
    ")\n",
    "\n",
    "training_df.show(4, truncate = False)\n",
    "validation_df.show(4, truncate = False) \n",
    "test_df.show(4, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017d6fd",
   "metadata": {},
   "source": [
    "## Alternating Least Square (ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS \n",
    "als = ALS ()\n",
    "\n",
    "# Reset the parameters for the ALS object. \n",
    "als.setPredictionCol(\"prediction\")\\\n",
    "    .setMaxIter (5)\\ \n",
    "    .setSeed (seed)\\ \n",
    "    .setRegParam(0.1)\\\n",
    "    .setUserCol('userId')\\\n",
    "    .setItemCol('movieId')\\\n",
    "    .setRatingCol('rating')\\\n",
    "    .setRank (8) #we got rank 8 as optimal\n",
    "\n",
    "# Create the model with these parameters. \n",
    "my_ratings_model = als.fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931c510",
   "metadata": {},
   "source": [
    "## Looking for RMSE again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48da8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "# it will essentially calculate the rmse score based on these columns\n",
    "reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "my_predict_df = my_ratings_model.transform(test_df)\n",
    "\n",
    "# Remove NaN values from prediction\n",
    "predicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n",
    "\n",
    "# Run the previously created RMSE evaluator, reg_eval, on the predicted_test_my_ratings_df DataFrame \n",
    "test_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\n",
    "print('The model had a RMSE on the test set of {0}'.format(test_RMSE_my_ratings))\n",
    "dbutils.widgets.text(\"input\", \"5\", \"\")\n",
    "ins=dbutils.widgets.get(\"input\")\n",
    "uid=int(ins)\n",
    "ll = predicted_test_my_ratings_df.filter(col(\"userId\") == uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovieRec= ll.join(movies_df, F.col('movieID') == F.col('ID')).drop('ID').select('title').take(10)\n",
    "\n",
    "l = dbutils.notebook.exit(MovieRec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e0c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2526e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a0925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e66de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6fffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b0c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae9531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb545be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269990a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d22197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb85719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde73585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08371c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ea738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f433b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
